{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.15: Supervised Learning - Neural Networks (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks are powerful models inspired by the structure of the human brain. The **Multi-Layer Perceptron (MLP)** is a classic type of feedforward neural network. It consists of an input layer, one or more hidden layers, and an output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Components\n",
    "- **Neurons (or Nodes)**: The basic computational units.\n",
    "- **Layers**: Neurons are organized into layers (input, hidden, output).\n",
    "- **Weights and Biases**: Parameters that the network learns during training.\n",
    "- **Activation Function**: A function (like ReLU or Sigmoid) applied to the output of each neuron, which allows the network to learn non-linear patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate non-linear data\n",
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "\n",
    "# Neural Networks are very sensitive to feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an MLP Classifier\n",
    "Scikit-learn's `MLPClassifier` provides a good starting point for understanding neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tyagi\\Documents\\Learning\\Data-Analyst-learning\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer_sizes: A tuple where each element is the number of neurons in that hidden layer.\n",
    "# e.g., (100,) means one hidden layer with 100 neurons.\n",
    "# e.g., (50, 25) means two hidden layers with 50 and 25 neurons.\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42)\n",
    "mlp_clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy on test set: {mlp_clf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Deep Learning\n",
    "While `MLPClassifier` is useful, for serious deep learning tasks (like image recognition, NLP), you should use more powerful and flexible libraries such as:\n",
    "- **TensorFlow** (with its high-level API, **Keras**)\n",
    "- **PyTorch**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
