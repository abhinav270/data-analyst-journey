{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6.3: Advanced Topics - Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced datasets are a common problem in classification where the classes are not represented equally. For example, in fraud detection, the number of fraudulent transactions is much smaller than legitimate ones. If not handled correctly, models can become biased towards the majority class and have poor performance on the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create an imbalanced dataset (95% class 0, 5% class 1)\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, \n",
    "                           n_redundant=0, n_classes=2, n_clusters_per_class=1, \n",
    "                           weights=[0.95, 0.05], flip_y=0, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem: A Naive Model\n",
    "Let's train a standard model and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       190\n",
      "           1       1.00      0.60      0.75        10\n",
      "\n",
      "    accuracy                           0.98       200\n",
      "   macro avg       0.99      0.80      0.87       200\n",
      "weighted avg       0.98      0.98      0.98       200\n",
      "\n",
      "Notice the poor recall for the minority class (1). The model is failing to identify it.\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Notice the poor recall for the minority class (1). The model is failing to identify it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Resampling with SMOTE\n",
    "Resampling techniques modify the training data to create a more balanced class distribution.\n",
    "- **Oversampling**: Increases the number of instances in the minority class. **SMOTE** (Synthetic Minority Over-sampling Technique) is a popular method that creates new synthetic samples.\n",
    "- **Undersampling**: Decreases the number of instances in the majority class.\n",
    "\n",
    "**Note**: This requires the `imbalanced-learn` library (`pip install -U imbalanced-learn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for demonstration and will NOT run without `imbalanced-learn` installed.\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import Pipeline\n",
    "\n",
    "# smote = SMOTE(random_state=42)\n",
    "# model_smote = LogisticRegression()\n",
    "\n",
    "# # It's best to use SMOTE within a pipeline to prevent data leakage\n",
    "# pipe = Pipeline([('smote', smote), ('model', model_smote)])\n",
    "# pipe.fit(X_train, y_train)\n",
    "# y_pred_smote = pipe.predict(X_test)\n",
    "\n",
    "# print(classification_report(y_test, y_pred_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Using Class Weights\n",
    "A simpler approach is to adjust the model's loss function to give more weight to the minority class. Many scikit-learn models have a `class_weight='balanced'` parameter that does this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       190\n",
      "           1       0.77      1.00      0.87        10\n",
      "\n",
      "    accuracy                           0.98       200\n",
      "   macro avg       0.88      0.99      0.93       200\n",
      "weighted avg       0.99      0.98      0.99       200\n",
      "\n",
      "\n",
      "Recall for class 1 has improved significantly.\n"
     ]
    }
   ],
   "source": [
    "model_weighted = LogisticRegression(class_weight='balanced')\n",
    "model_weighted.fit(X_train, y_train)\n",
    "y_pred_weighted = model_weighted.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_weighted))\n",
    "print(\"\\nRecall for class 1 has improved significantly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
